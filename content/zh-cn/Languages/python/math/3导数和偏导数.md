---
title: "3. 导数和偏导数"
linkTitle: "导数和偏导数"
weight: 4
description: >
  导数和偏导数
---

{{% pageinfo %}}
导数和偏导数
{{% /pageinfo %}}


# 导数和偏导数

- 导数
- 梯度
- 矩阵微积分
- 最优化方法(梯度下降、牛顿法等)




导数是一个函数某一点的变化率，对于单变量函数f(x)，导数记作 f'(x) 或 df/dx。

偏导数是一个函数有多个变量时，比如f(x,y)，对每个变量进行微分，得到偏导数。

梯度：是多变量函数的导数向量，对应每个变量的偏导数，对于函数f(x,y,z)，梯度记作 grad f(x,y,z) = (df/dx, df/dy, df/dz)。
梯度指向函数增长最快的反向，梯度的大小（即向量的长度）表示增长率的快慢。


矩阵微积分：对矩阵求导，得到矩阵的雅可比矩阵，即梯度的矩阵形式。

海森矩阵是一个方阵，包含二阶偏导数，通常用户分析函数的曲率。


最优化方法：梯度下降、牛顿法等。

### 梯度下降
梯度下降：是一种迭代优化方法，用于找到函数的局部或者全局最小值。

算法步骤：
1. 随机选择一个起始点x0。
2. 计算函数f(x)在x0处的梯度grad f(x0)。
3. 确定步长α，使得x1 = x0 - α*grad f(x0)。
4. 重复步骤2和步骤3，直到收敛。

### 牛顿法
牛顿法利用二阶导数（海森矩阵）加快收敛速度，适用于函数曲率信息明显的情况。

算法步骤：
1. 随机选择一个起始点x0。
2. 计算函数f(x)在x0处的海森矩阵H(x0)。
3. 计算函数f(x)在x0处的梯度grad f(x0)。
4. 计算矩阵H(x0)的逆矩阵，即H(x0)^-1。
5. 确定步长α，使得x1 = x0 - α*H(x0)^-1*grad f(x0)。
6. 重复步骤2-5，直到收敛。

牛顿法需要计算和存储海森矩阵，计算成本较高。





