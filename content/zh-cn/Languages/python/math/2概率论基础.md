---
title: "2. 概率论基础"
linkTitle: "概率论基础"
weight: 3
description: >
  概率论基础
---

{{% pageinfo %}}
概率论基础
{{% /pageinfo %}}


# 概率论基础

## 概率论与统计学大纲
- 基本概念
  - 概率与条件概率
  - 随机变量与分布（离散和连续）
  - 期望、方差和标准差
- 主要分布
  - 二项分布与泊松分布
  - 正态分布与均匀分布
  - 指数分布与Gamma分布
- 描述性统计
  - 均值、中位数和众数
  - 分位数与箱线图
  - 方差与标准差
- 统计推断
  - 点估计与区间估计
  - 假设检验与p值
  - 置信区间
- 贝叶斯统计
  - 贝叶斯定理
  - 先验和后验分布
  - 贝叶斯统计与推断


## 基本概念与分布情况
一个概率空间由三元组定义：
- 状态空间/样本空间 ： 一个试验所有可能出现的结果
- 事件空间 ： 试验的每一个单一结果为一个事件，它是状态空间的子集，而事件空间就是所有事件构成的集合。
- 概率P(A) ：描述发生的可能性，也称概率函数。

举个例子来理解上述三个概念。假如我们投掷一个6面骰子，那么样本空间 Ω = {1,2,3,4,5,6}。如果我们关注的事件是骰子点数是奇数还是偶数，那么事件空间就是 F = {∅,{1,3,5},{2,4,6}}


随机变量：
- 定义：设 X 为一个试验，其结果为一个实数值，则 X 称为随机变量。
- 随机变量的分布：随机变量 X 的分布是指随机变量 X 取各个值的概率。


根据状态空间不同，随机变量可以分为离散和连续的。
- 能取有限的值，则该随机变量是离散的，离散随机变量的概率分布通过**概率质量函数**（PMF）表示，记作P(X=x)
- 能取无限多个值，则该随机变量是连续的，连续随机变量的概率分布通过**概率密度函数**（PDF）表示，记作f(x)



分布：
- 概率分布：随机变量取一个特定值的概率， P(X)
- 边缘分布：一个随机变量对于其自身的概率分布。换句话说，就是在联合分布的情况下，边缘分布就是把另一个变量的所有可能取值相加。
- 联合分布：由多余一个变量决定的概率分布，即多件事件同时发生的情况， P(X,Y)
- 条件分布：已知某些事件已经发生的前提下，另一事件发生的概率的分布。P(X|Y)

- 二项分布：在离散时间上，某事件发生的次数的分布。有两个参数，n，表示试验次数，p表示事件发生的概率。P(X=k) = C(n,k)p^k(1-p)^(n-k) ，其中C(n,k)是组合数，读作n选k，公式 n! / (k!(n-k)!) ，如 C(10,3) = 10!/(3! * 7!)
- 泊松分布：在连续时间上，某事件发生的次数的分布。只有一个参数，λ，表示事件发生的次数。P(X=k) = e^(-λ)λ^k/k! ，e是自然对数的底，约等于2.71828

- 正态分布/高斯分布：常见的连续概率分布，在连续时间上，一个随机变量的分布。有两个参数，μ表示分布的均值，σ^2表示方差。P(X=x) = (2πσ^2)^{-1/2}exp(-(x-μ)^2/2σ^2)
- 多项分布：描述多个类别事件的离散概率分布。有两个参数，n，表示试验次数，p1,p2,...,pk表示各个类别事件发生的概率。次数n 和 每个类别的概率向量 p。公式 P(X1=x1,X2=x2,...,Xk=xk) = (n! / x1!x2!...xk!) * p1^x1 * p2^x2 * ... * pk^xk

总结：
1. 高斯分布 主要用于描述连续数据的概率分布，定义了均值和标准差。
2. 多项分布 则用于描述多个类别的离散数据，在多个类别中统计每个类别出现的次数。


## 统计推断
从样本数据中获取信息并对总体进行预测和推断，主要设计两大部分：点估计和区间估计。


点估计：用一个单一的数值从样本数据中去估计总体参数。
- 均值估计：用样本均值去估计总体均值
- 方差估计：用样本方差去估计总体方差

如有一个样本数据集{x1,x2,...,xn}，样本均值的点估计计算方法是：
> x̄ = 1/n ∑(n,i=1) xi

区间估计：用一组数值从样本数据中去估计总体参数的置信区间。

置信区间：表示在一定的置信水平下，总体参数位于某个范围内。

置信水平：置信水平是指总体参数的真实值与估计值的差距所能容忍的最大值。


## 贝叶斯统计
贝叶斯统计是一种概率论的分支，它利用贝叶斯定理来更新概率估计，在获取新数据时动态调整对未知参数或事件的概率分布。与传统频率派统计不同，贝叶斯统计将概率解释为对某一事件的信心程度，而不仅仅是长期频率。

核心概念：
- 先验概率：在考虑新数据之前，对位未知参数的初始估计。
- 似然函数：根据现有数据计算出，表示数据在特定参数值下出现的概率。
- 后验概率：更新后的概率，结合了先验概率和新数据，反映了在新数据下，参数的估计值。

贝叶斯定理：
- 贝叶斯定理是关于概率的基本定理，它告诉我们如何利用已知的信息来更新不完整的概率分布。
- 贝叶斯定理的基本思想是，已知某件事情发生的概率，可以根据已知的信息来计算另一件事情发生的概率。
- 贝叶斯定理的公式为：P(A|B) = (P(B|A)P(A))/P(B)


**例子说明：**

Q: 假设我们做市场调查，想知道城市中有多少百分比的人喜欢某种饮料。

1. 先验概率：我们不知道城市中有多少百分比的人喜欢某种饮料，根据其他城市的经验估计有20%的人会喜欢。
2. 收集数据：我们随机抽取了100个人，其中25人喜欢
3. 计算似然函数：对于每个可能的喜欢百分比（从0%到100％），计算这些数据出现的概率。这个概率跟二项分布有关，因此可以用二项公式计算每个可能喜欢百分比下观测到25/100这个结果的概率。
4. 应用贝叶斯定理：结合先验概率和似然函数来计算后验概率。计算后，会得到一个更新的概率分布，表示在观测到25个喜欢的人之后，对喜欢百分比的新的信心。


应用：
- 机器学习： 如贝叶斯网络，贝叶斯优化，贝叶斯分类器等
- 医学：
- 经济金融：
- 自然科学


### 似然函数的解释：
似然函数是在给定参数值下，数据出现的概率。它描述了在假设某个特定参数值时，观察到给定数据的可能性。虽然它和概率密度函数（PDF）/概率质量函数（PMF）有相似之处，但在似然函数中，把参数视为变量，而数据是固定的。


如抛了10次硬币，观测到7次正面朝上，你想估计 𝜃 是多少。

n=10,k=7，函数是 L(𝜃) = c(10 7)𝜃^7(1-𝜃)^3，其中 c(10 7) 是组合数。

为了找到最有可能的 θ，我们通常会最大化这个似然函数（这叫做最大似然估计，MLE）。在实际操作中，这可以通过对数似然函数来简化计算。

直观理解：
- 固定数据，变参数：似然函数固定了数据（如观测到的7正3反），然后变换参数（如 𝜃=0.1,0.2,…,0.9θ=0.1,0.2,…,0.9 等）来看在不同 θ 下这些数据出现的可能性。
- 找到最优θ：似然函数的最大值对应的参数值 𝜃 就是我们认为最有可能使这些数据出现的参数值。